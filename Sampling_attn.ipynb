{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tannisthamaiti/DiffusionModels_DDPM_DDIM/blob/main/Sampling_attn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "RXq_Eih9i5H9"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, Tuple\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models, transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation, PillowWriter\n",
        "import numpy as np\n",
        "from IPython.display import HTML\n",
        "#from diffusion_utils import *\n",
        "#from spatial_helper import *"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Copied the diffusion_utils block of codes here"
      ],
      "metadata": {
        "id": "RzegH7sIPn6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torchvision.utils import save_image, make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation, PillowWriter\n",
        "import os\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class ResidualConvBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_channels: int, out_channels: int, is_res: bool = False\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        # Check if input and output channels are the same for the residual connection\n",
        "        self.same_channels = in_channels == out_channels\n",
        "\n",
        "        # Flag for whether or not to use residual connection\n",
        "        self.is_res = is_res\n",
        "\n",
        "        # First convolutional layer\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),   # 3x3 kernel with stride 1 and padding 1\n",
        "            nn.BatchNorm2d(out_channels),   # Batch normalization\n",
        "            nn.GELU(),   # GELU activation function\n",
        "        )\n",
        "\n",
        "        # Second convolutional layer\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),   # 3x3 kernel with stride 1 and padding 1\n",
        "            nn.BatchNorm2d(out_channels),   # Batch normalization\n",
        "            nn.GELU(),   # GELU activation function\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        # If using residual connection\n",
        "        if self.is_res:\n",
        "            # Apply first convolutional layer\n",
        "            x1 = self.conv1(x)\n",
        "\n",
        "            # Apply second convolutional layer\n",
        "            x2 = self.conv2(x1)\n",
        "\n",
        "            # If input and output channels are the same, add residual connection directly\n",
        "            if self.same_channels:\n",
        "                out = x + x2\n",
        "            else:\n",
        "                # If not, apply a 1x1 convolutional layer to match dimensions before adding residual connection\n",
        "                shortcut = nn.Conv2d(x.shape[1], x2.shape[1], kernel_size=1, stride=1, padding=0).to(x.device)\n",
        "                out = shortcut(x) + x2\n",
        "            #print(f\"resconv forward: x {x.shape}, x1 {x1.shape}, x2 {x2.shape}, out {out.shape}\")\n",
        "\n",
        "            # Normalize output tensor\n",
        "            return out / 1.414\n",
        "\n",
        "        # If not using residual connection, return output of second convolutional layer\n",
        "        else:\n",
        "            x1 = self.conv1(x)\n",
        "            x2 = self.conv2(x1)\n",
        "            return x2\n",
        "\n",
        "    # Method to get the number of output channels for this block\n",
        "    def get_out_channels(self):\n",
        "        return self.conv2[0].out_channels\n",
        "\n",
        "    # Method to set the number of output channels for this block\n",
        "    def set_out_channels(self, out_channels):\n",
        "        self.conv1[0].out_channels = out_channels\n",
        "        self.conv2[0].in_channels = out_channels\n",
        "        self.conv2[0].out_channels = out_channels"
      ],
      "metadata": {
        "id": "Yf67Qv5xPjea"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnetUp(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UnetUp, self).__init__()\n",
        "\n",
        "        # Create a list of layers for the upsampling block\n",
        "        # The block consists of a ConvTranspose2d layer for upsampling, followed by two ResidualConvBlock layers\n",
        "        layers = [\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, 2, 2),\n",
        "            ResidualConvBlock(out_channels, out_channels),\n",
        "            ResidualConvBlock(out_channels, out_channels),\n",
        "        ]\n",
        "\n",
        "        # Use the layers to create a sequential model\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        # Concatenate the input tensor x with the skip connection tensor along the channel dimension\n",
        "        x = torch.cat((x, skip), 1)\n",
        "\n",
        "        # Pass the concatenated tensor through the sequential model and return the output\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UnetDown(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UnetDown, self).__init__()\n",
        "\n",
        "        # Create a list of layers for the downsampling block\n",
        "        # Each block consists of two ResidualConvBlock layers, followed by a MaxPool2d layer for downsampling\n",
        "        layers = [ResidualConvBlock(in_channels, out_channels), ResidualConvBlock(out_channels, out_channels), nn.MaxPool2d(2)]\n",
        "\n",
        "        # Use the layers to create a sequential model\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass the input through the sequential model and return the output\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "GMhLRUB4RN3U"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, sfilename, lfilename, transform, null_context=False):\n",
        "        self.sprites = np.load(sfilename)\n",
        "        self.slabels = np.load(lfilename)\n",
        "        print(f\"sprite shape: {self.sprites.shape}\")\n",
        "        print(f\"labels shape: {self.slabels.shape}\")\n",
        "        self.transform = transform\n",
        "        self.null_context = null_context\n",
        "        self.sprites_shape = self.sprites.shape\n",
        "        self.slabel_shape = self.slabels.shape\n",
        "\n",
        "    # Return the number of images in the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.sprites)\n",
        "\n",
        "    # Get the image and label at a given index\n",
        "    def __getitem__(self, idx):\n",
        "        # Return the image and label as a tuple\n",
        "        if self.transform:\n",
        "            image = self.transform(self.sprites[idx])\n",
        "            if self.null_context:\n",
        "                label = torch.tensor(0).to(torch.int64)\n",
        "            else:\n",
        "                label = torch.tensor(self.slabels[idx]).to(torch.int64)\n",
        "        return (image, label)\n",
        "\n",
        "    def getshapes(self):\n",
        "        # return shapes of data and labels\n",
        "        return self.sprites_shape, self.slabel_shape\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),                # from [0,255] to range [0.0,1.0]\n",
        "    transforms.Normalize((0.5,), (0.5,))  # range [-1,1]\n",
        "\n",
        "])\n",
        "\n",
        "def get_backbone(name, pretrained=True):\n",
        "\n",
        "    \"\"\" Loading backbone, defining names for skip-connections and encoder output. \"\"\"\n",
        "\n",
        "    # TODO: More backbones\n",
        "     # loading backbone model\n",
        "    if name == 'resnet18':\n",
        "        backbone = models.resnet18(pretrained=pretrained)\n",
        "    elif name == 'resnet34':\n",
        "        backbone = models.resnet34(pretrained=pretrained)\n",
        "    else:\n",
        "        raise NotImplemented('{} backbone model is not implemented so far.'.format(name))\n",
        "    if name.startswith('resnet'):\n",
        "        feature_names = [None, 'relu', 'layer1', 'layer2', 'layer3']\n",
        "        backbone_output = 'layer4'\n",
        "    else:\n",
        "        raise NotImplemented('{} backbone model is not implemented so far.'.format(name))\n",
        "\n",
        "    return backbone, feature_names, backbone_output"
      ],
      "metadata": {
        "id": "9FCYNY1TRcbr"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbedFC(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim):\n",
        "        super(EmbedFC, self).__init__()\n",
        "        '''\n",
        "        This class defines a generic one layer feed-forward neural network for embedding input data of\n",
        "        dimensionality input_dim to an embedding space of dimensionality emb_dim.\n",
        "        '''\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        # define the layers for the network\n",
        "        layers = [\n",
        "            nn.Linear(input_dim, emb_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(emb_dim, emb_dim),\n",
        "        ]\n",
        "\n",
        "        # create a PyTorch sequential model consisting of the defined layers\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # flatten the input tensor\n",
        "        x = x.view(-1, self.input_dim)\n",
        "        # apply the model layers to the flattened tensor\n",
        "        return self.model(x)\n",
        "\n",
        "def unorm(x):\n",
        "    # unity norm. results in range of [0,1]\n",
        "    # assume x (h,w,3)\n",
        "    xmax = x.max((0,1))\n",
        "    xmin = x.min((0,1))\n",
        "    return(x - xmin)/(xmax - xmin)\n",
        "\n",
        "def norm_all(store, n_t, n_s):\n",
        "    # runs unity norm on all timesteps of all samples\n",
        "    nstore = np.zeros_like(store)\n",
        "    for t in range(n_t):\n",
        "        for s in range(n_s):\n",
        "            nstore[t,s] = unorm(store[t,s])\n",
        "    return nstore\n",
        "\n",
        "def norm_torch(x_all):\n",
        "    # runs unity norm on all timesteps of all samples\n",
        "    # input is (n_samples, 3,h,w), the torch image format\n",
        "    x = x_all.cpu().numpy()\n",
        "    xmax = x.max((2,3))\n",
        "    xmin = x.min((2,3))\n",
        "    xmax = np.expand_dims(xmax,(2,3))\n",
        "    xmin = np.expand_dims(xmin,(2,3))\n",
        "    nstore = (x - xmin)/(xmax - xmin)\n",
        "    return torch.from_numpy(nstore)\n",
        "\n",
        "def gen_tst_context(n_cfeat):\n",
        "    \"\"\"\n",
        "    Generate test context vectors\n",
        "    \"\"\"\n",
        "    vec = torch.tensor([\n",
        "    [1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1],  [0,0,0,0,0],      # human, non-human, food, spell, side-facing\n",
        "    [1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1],  [0,0,0,0,0],      # human, non-human, food, spell, side-facing\n",
        "    [1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1],  [0,0,0,0,0],      # human, non-human, food, spell, side-facing\n",
        "    [1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1],  [0,0,0,0,0],      # human, non-human, food, spell, side-facing\n",
        "    [1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1],  [0,0,0,0,0],      # human, non-human, food, spell, side-facing\n",
        "    [1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0], [0,0,0,1,0], [0,0,0,0,1],  [0,0,0,0,0]]      # human, non-human, food, spell, side-facing\n",
        "    )\n",
        "    return len(vec), vec\n",
        "\n",
        "def plot_grid(x,n_sample,n_rows,save_dir,w):\n",
        "    # x:(n_sample, 3, h, w)\n",
        "    ncols = n_sample//n_rows\n",
        "    grid = make_grid(norm_torch(x), nrow=ncols)  # curiously, nrow is number of columns.. or number of items in the row.\n",
        "    save_image(grid, save_dir + f\"run_image_w{w}.png\")\n",
        "    print('saved image at ' + save_dir + f\"run_image_w{w}.png\")\n",
        "    return grid\n",
        "\n",
        "def plot_sample(x_gen_store,n_sample,nrows,save_dir, fn,  w, save=False):\n",
        "    ncols = n_sample//nrows\n",
        "    sx_gen_store = np.moveaxis(x_gen_store,2,4)                               # change to Numpy image format (h,w,channels) vs (channels,h,w)\n",
        "    nsx_gen_store = norm_all(sx_gen_store, sx_gen_store.shape[0], n_sample)   # unity norm to put in range [0,1] for np.imshow\n",
        "\n",
        "    # create gif of images evolving over time, based on x_gen_store\n",
        "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, sharex=True, sharey=True,figsize=(ncols,nrows))\n",
        "    def animate_diff(i, store):\n",
        "        print(f'gif animating frame {i} of {store.shape[0]}', end='\\r')\n",
        "        plots = []\n",
        "        for row in range(nrows):\n",
        "            for col in range(ncols):\n",
        "                axs[row, col].clear()\n",
        "                axs[row, col].set_xticks([])\n",
        "                axs[row, col].set_yticks([])\n",
        "                plots.append(axs[row, col].imshow((store[i,(row*ncols)+col])))\n",
        "        return plots\n",
        "    ani = FuncAnimation(fig, animate_diff, fargs=[nsx_gen_store],  interval=200, blit=False, repeat=True, frames=nsx_gen_store.shape[0])\n",
        "    plt.close()\n",
        "    if save:\n",
        "        ani.save(save_dir + f\"{fn}_w{w}.gif\", dpi=100, writer=PillowWriter(fps=5))\n",
        "        print('saved gif at ' + save_dir + f\"{fn}_w{w}.gif\")\n",
        "    return ani\n"
      ],
      "metadata": {
        "id": "UkoRFqBCQ2_-"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Copied code from spatial_helper"
      ],
      "metadata": {
        "id": "0TaX8NHvQis-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops import rearrange\n",
        "import math\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "  def __init__(self, embed_dim, hidden_dim, context_dim=None, num_heads=1,):\n",
        "    \"\"\"\n",
        "    Note: For simplicity reason, we just implemented 1-head attention.\n",
        "    Feel free to implement multi-head attention! with fancy tensor manipulations.\n",
        "    \"\"\"\n",
        "    super(CrossAttention, self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.context_dim = context_dim\n",
        "    self.embed_dim = embed_dim\n",
        "    self.query = nn.Linear(hidden_dim, embed_dim, bias=False)\n",
        "    if context_dim is None:\n",
        "      self.self_attn = True\n",
        "      self.key = nn.Linear(hidden_dim, embed_dim, bias=False)     ###########\n",
        "      self.value = nn.Linear(hidden_dim, hidden_dim, bias=False)  ############\n",
        "    else:\n",
        "      self.self_attn = False\n",
        "      self.key = nn.Linear(context_dim, embed_dim, bias=False)   #############\n",
        "      self.value = nn.Linear(context_dim, embed_dim, bias=False) ############\n",
        "\n",
        "\n",
        "  def forward(self, tokens, context=None):\n",
        "    # tokens: with shape [batch, sequence_len, hidden_dim]\n",
        "    # context: with shape [batch, contex_seq_len, context_dim]\n",
        "    if self.self_attn:\n",
        "        Q = self.query(tokens)\n",
        "        K = self.key(tokens)\n",
        "        V = self.value(tokens)\n",
        "    else:\n",
        "        # implement Q, K, V for the Cross attention\n",
        "        Q = self.query(context)\n",
        "        K = self.key(context)\n",
        "        V = self.value(context)\n",
        "\n",
        "    #print(Q.shape, K.shape, V.shape)\n",
        "    ####### YOUR CODE HERE (2 lines)\n",
        "    scoremats = torch.einsum(\"BTD,BSD->BTS\", Q, K)         # inner product of Q and K, a tensor\n",
        "    attnmats = torch.nn.functional.softmax(scoremats / math.sqrt(self.embed_dim), dim=-1)          # softmax of scoremats\n",
        "    #print(scoremats.shape, attnmats.shape, )\n",
        "    ctx_vecs = torch.einsum(\"BTS,BSH->BTH\", attnmats, V)  # weighted average value vectors by attnmats\n",
        "    return ctx_vecs\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "  \"\"\"The transformer block that combines self-attn, cross-attn and feed forward neural net\"\"\"\n",
        "  def __init__(self, hidden_dim, context_dim):\n",
        "    super(TransformerBlock, self).__init__()\n",
        "    self.attn_self = CrossAttention(hidden_dim, hidden_dim, )\n",
        "    self.attn_cross = CrossAttention(hidden_dim, hidden_dim, context_dim)\n",
        "\n",
        "    self.norm1 = nn.LayerNorm(hidden_dim)\n",
        "    self.norm2 = nn.LayerNorm(hidden_dim)\n",
        "    self.norm3 = nn.LayerNorm(hidden_dim)\n",
        "    # implement a 2 layer MLP with K*hidden_dim hidden units, and nn.GeLU nonlinearity #######\n",
        "    self.ffn  = nn.Sequential(nn.Linear(hidden_dim, 3 * hidden_dim),nn.GELU(), nn.Linear(3 * hidden_dim, hidden_dim))\n",
        "\n",
        "  def forward(self, x, context=None):\n",
        "    # Notice the + x as residue connections\n",
        "    x = self.attn_self(self.norm1(x)) + x\n",
        "    # Notice the + x as residue connections\n",
        "    x = self.attn_cross(self.norm2(x), context=context) + x\n",
        "    # Notice the + x as residue connections\n",
        "    x = self.ffn(self.norm3(x)) + x\n",
        "    return x\n",
        "\n",
        "class SpatialTransformer(nn.Module):\n",
        "  def __init__(self, hidden_dim, context_dim):\n",
        "    super(SpatialTransformer, self).__init__()\n",
        "    self.transformer = TransformerBlock(hidden_dim, context_dim)\n",
        "\n",
        "  def forward(self, x, context=None):\n",
        "    b, c, h, w = x.shape\n",
        "    x_in = x\n",
        "    # Combine the spatial dimensions and move the channel dimen to the end\n",
        "    x = rearrange(x, \"b c h w->b (h w) c\")\n",
        "    # Apply the sequence transformer\n",
        "    x = self.transformer(x, context)\n",
        "    # Reverse the process\n",
        "    x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
        "    # Residue\n",
        "    return x + x_in"
      ],
      "metadata": {
        "id": "ou3j0e94Qoni"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "6hK-ONvei5IF"
      },
      "outputs": [],
      "source": [
        "def get_backbone(name, pretrained=True):\n",
        "\n",
        "    \"\"\" Loading backbone, defining names for skip-connections and encoder output. \"\"\"\n",
        "\n",
        "    # TODO: More backbones\n",
        "     # loading backbone model\n",
        "    if name == 'resnet18':\n",
        "        backbone = models.resnet18(weights=pretrained)\n",
        "    elif name == 'resnet34':\n",
        "        backbone = models.resnet34(weights=pretrained)\n",
        "    else:\n",
        "        raise NotImplemented('{} backbone model is not implemented so far.'.format(name))\n",
        "    if name.startswith('resnet'):\n",
        "        feature_names = [None, 'relu', 'layer1', 'layer2', 'layer3']\n",
        "        backbone_onebefore_output = 'layer3'\n",
        "        backbone_output = 'layer4'\n",
        "    else:\n",
        "        raise NotImplemented('{} backbone model is not implemented so far.'.format(name))\n",
        "\n",
        "    return backbone.to(device), feature_names, backbone_output, backbone_onebefore_output\n",
        "\n",
        "\n",
        "class UpsampleBlock(nn.Module):\n",
        "\n",
        "    # TODO: separate parametric and non-parametric classes?\n",
        "    # TODO: skip connection concatenated OR added\n",
        "\n",
        "    def __init__(self, ch_in, ch_out=None, skip_in=0, use_bn=True, parametric=False):\n",
        "        super(UpsampleBlock, self).__init__()\n",
        "\n",
        "        self.parametric = parametric\n",
        "        ch_out = ch_in/2 if ch_out is None else ch_out\n",
        "         # first convolution: either transposed conv, or conv following the skip connection\n",
        "        if parametric:\n",
        "            # versions: kernel=4 padding=1, kernel=2 padding=0\n",
        "            self.up = nn.ConvTranspose2d(in_channels=ch_in, out_channels=ch_out, kernel_size=(4, 4),\n",
        "                                         stride=2, padding=1, output_padding=0, bias=(not use_bn))\n",
        "            self.bn1 = nn.BatchNorm2d(ch_out) if use_bn else None\n",
        "        else:\n",
        "            self.up = None\n",
        "            ch_in = ch_in + skip_in\n",
        "            self.conv1 = nn.Conv2d(in_channels=ch_in, out_channels=ch_out, kernel_size=(3, 3),\n",
        "                                   stride=1, padding=1, bias=(not use_bn))\n",
        "            self.bn1 = nn.BatchNorm2d(ch_out) if use_bn else None\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        # second convolution\n",
        "        conv2_in = ch_out if not parametric else ch_out + skip_in\n",
        "        self.conv2 = nn.Conv2d(in_channels=conv2_in, out_channels=ch_out, kernel_size=(3, 3),\n",
        "                               stride=1, padding=1, bias=(not use_bn))\n",
        "        self.bn2 = nn.BatchNorm2d(ch_out) if use_bn else None\n",
        "    def forward(self, x, skip_connection=None):\n",
        "\n",
        "        x = self.up(x) if self.parametric else F.interpolate(x, size=None, scale_factor=2, mode='bilinear',\n",
        "                                                             align_corners=None)\n",
        "        if self.parametric:\n",
        "            x = self.bn1(x) if self.bn1 is not None else x\n",
        "            x = self.relu(x)\n",
        "\n",
        "        if skip_connection is not None:\n",
        "            x = torch.cat([x, skip_connection], dim=1)\n",
        "\n",
        "        if not self.parametric:\n",
        "            x = self.conv1(x)\n",
        "            x = self.bn1(x) if self.bn1 is not None else x\n",
        "            x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x) if self.bn2 is not None else x\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "class Unet(nn.Module):\n",
        "\n",
        "    \"\"\" U-Net (https://arxiv.org/pdf/1505.04597.pdf) implementation with pre-trained torchvision backbones.\"\"\"\n",
        "\n",
        "    def __init__(self,in_channels,n_feat, c_feat=None,\n",
        "                 backbone_name='resnet50',\n",
        "                 pretrained=True,\n",
        "                 encoder_freeze=False,\n",
        "                 classes=21,\n",
        "                 decoder_filters=(256, 128, 64, 32, 16),\n",
        "                 parametric_upsampling=True,\n",
        "                 shortcut_features='default',\n",
        "                 decoder_use_batchnorm=True):\n",
        "        super(Unet, self).__init__()\n",
        "\n",
        "        self.backbone_name = backbone_name\n",
        "        self.n_feat = n_feat\n",
        "        self.n_cfeat = 5\n",
        "        self.in_channels= in_channels\n",
        "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
        "\n",
        "        self.backbone, self.shortcut_features, self.bb_out_name, self.bb_onebefore_out_name = get_backbone(backbone_name, pretrained=pretrained)\n",
        "        self.attn3 = SpatialTransformer(n_feat,None)\n",
        "\n",
        "\n",
        "        # build decoder part\n",
        "        self.upsample_blocks = nn.ModuleList()\n",
        "        decoder_filters = decoder_filters[:len(self.shortcut_features)]  # avoiding having more blocks than skip connections\n",
        "\n",
        "        num_blocks = len(self.shortcut_features)\n",
        "        # Embed the timestep and context labels with a one-layer fully connected neural network\n",
        "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
        "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
        "        self.up0 = nn.ConvTranspose2d(in_channels=2*n_feat, out_channels=2*n_feat, kernel_size=(8, 8),\n",
        "                                         stride=8, padding=0, output_padding=0)\n",
        "        self.up00 = nn.ConvTranspose2d(in_channels=256, out_channels=256, kernel_size=8, stride=8, padding=0, bias=False)\n",
        "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
        "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
        "\n",
        "        self.to_vec = nn.Sequential(nn.AvgPool2d((1)), nn.GELU())\n",
        "        # Initialize the final convolutional layers to map to the same number of channels as the input image\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1), # reduce number of feature maps   #in_channels, out_channels, kernel_size, stride=1, padding=0\n",
        "            nn.GroupNorm(8, n_feat), # normalize\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1), # map to same number of channels as input\n",
        "        )\n",
        "\n",
        "        self.final_conv = nn.Conv2d(decoder_filters[-1], classes, kernel_size=(1, 1))\n",
        "\n",
        "\n",
        "        if encoder_freeze:\n",
        "            self.freeze_encoder()\n",
        "\n",
        "        self.replaced_conv1 = False  # for accommodating  inputs with different number of channels later\n",
        "\n",
        "    def freeze_encoder(self):\n",
        "\n",
        "        \"\"\" Freezing encoder parameters, the newly initialized decoder parameters are remaining trainable. \"\"\"\n",
        "\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, input,t=None, c=None):\n",
        "\n",
        "        \"\"\" Forward propagation in U-Net. \"\"\"\n",
        "\n",
        "        x_init = self.init_conv(input)\n",
        "        x, features = self.forward_backbone(input)\n",
        "        down1,features_layer3= self.forward_onebefore_backbone(input)\n",
        "        ## SpatialTransformer applied to down1\n",
        "        down_attn=self.attn3(down1)\n",
        "\n",
        "\n",
        "\n",
        "        #print(f\"input: {input.shape}\")\n",
        "        # hiddenvec\n",
        "        hiddenvec = self.to_vec(x)\n",
        "        down2=self.up0(hiddenvec)\n",
        "        down1=self.up00(down_attn)\n",
        "\n",
        "        # embed context and timestep\n",
        "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
        "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
        "        #print(f\"down1: {down1.shape}\")\n",
        "        up1 = self.up0(hiddenvec)\n",
        "        cemb1= torch.empty(batch_size, 512, 1, 1).normal_().to(device)\n",
        "        cemb2= torch.empty(batch_size, 256, 1, 1).normal_().to(device)\n",
        "        #down1: torch.Size([32, 256, 64, 64])\n",
        "\n",
        "        up2 = self.up1(cemb1*up1+temb1,down2)\n",
        "        up2_attn= self.attn3(up2)\n",
        "\n",
        "        up3 = self.up2(cemb2*up2_attn + temb2, down1)\n",
        "        out = self.out(torch.cat((up3, x_init), 1))\n",
        "        return out\n",
        "\n",
        "\n",
        "    def forward_backbone(self, x):\n",
        "\n",
        "        \"\"\" Forward propagation in backbone encoder network.  \"\"\"\n",
        "\n",
        "        features = {None: None} if None in self.shortcut_features else dict()\n",
        "        for name, child in self.backbone.named_children():\n",
        "            x = child(x)\n",
        "            if name in self.shortcut_features:\n",
        "                features[name] = x\n",
        "            if name == self.bb_out_name:\n",
        "                break\n",
        "\n",
        "        return x, features\n",
        "    def forward_onebefore_backbone(self, x):\n",
        "\n",
        "        \"\"\" Forward propagation in backbone encoder network.  \"\"\"\n",
        "\n",
        "        features = {None: None} if None in self.shortcut_features else dict()\n",
        "        for name, child in self.backbone.named_children():\n",
        "            x = child(x)\n",
        "            if name in self.shortcut_features:\n",
        "                features[name] = x\n",
        "            if name == self.bb_onebefore_out_name:\n",
        "                break\n",
        "\n",
        "        return x, features\n",
        "\n",
        "    def infer_skip_channels(self):\n",
        "\n",
        "        \"\"\" Getting the number of channels at skip connections and at the output of the encoder. \"\"\"\n",
        "\n",
        "        x = torch.zeros(1, 3, 224, 224)\n",
        "        has_fullres_features = self.backbone_name.startswith('vgg') or self.backbone_name == 'unet_encoder'\n",
        "        channels = [] if has_fullres_features else [0]  # only VGG has features at full resolution\n",
        "\n",
        "        # forward run in backbone to count channels (dirty solution but works for *any* Module)\n",
        "        for name, child in self.backbone.named_children():\n",
        "            x = child(x)\n",
        "            if name in self.shortcut_features:\n",
        "                channels.append(x.shape[1])\n",
        "            if name == self.bb_out_name:\n",
        "                out_channels = x.shape[1]\n",
        "                break\n",
        "        return channels, out_channels\n",
        "\n",
        "    def get_pretrained_parameters(self):\n",
        "        for name, param in self.backbone.named_parameters():\n",
        "            if not (self.replaced_conv1 and name == 'conv1.weight'):\n",
        "                yield param\n",
        "\n",
        "    def get_random_initialized_parameters(self):\n",
        "        pretrained_param_names = set()\n",
        "        for name, param in self.backbone.named_parameters():\n",
        "            if not (self.replaced_conv1 and name == 'conv1.weight'):\n",
        "                pretrained_param_names.add('backbone.{}'.format(name))\n",
        "\n",
        "        for name, param in self.named_parameters():\n",
        "            if name not in pretrained_param_names:\n",
        "                yield param\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "AT4GaCmVi5IO"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "\n",
        "# diffusion hyperparameters\n",
        "timesteps = 500\n",
        "beta1 = 1e-4 #0.9\n",
        "beta2 = 0.02 #0.99\n",
        "\n",
        "# network hyperparameters\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else torch.device('cpu'))\n",
        "n_feat = 256 # 64 hidden dimension feature\n",
        "n_cfeat = 5 # context vector is of size 5\n",
        "height = 128 # 16x16 image\n",
        "save_dir = 'weights_attn/'\n",
        "\n",
        "# training hyperparameters\n",
        "batch_size = 32\n",
        "n_epoch = 100\n",
        "lrate=1e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "22AbezBDi5IQ"
      },
      "outputs": [],
      "source": [
        "def _warmup_beta(beta_start, beta_end, num_diffusion_timesteps, warmup_frac):\n",
        "  betas = beta_end * np.ones(num_diffusion_timesteps, dtype=np.float64)\n",
        "  warmup_time = int(num_diffusion_timesteps * warmup_frac)\n",
        "  betas[:warmup_time] = np.linspace(beta_start, beta_end, warmup_time, dtype=np.float64)\n",
        "  return betas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "_K9wltgki5IR"
      },
      "outputs": [],
      "source": [
        "def get_beta_schedule(beta_schedule, beta_start, beta_end, num_diffusion_timesteps):\n",
        "  if beta_schedule == 'quad':\n",
        "    betas = np.linspace(beta_start ** 0.5, beta_end ** 0.5, num_diffusion_timesteps, dtype=np.float64) ** 2\n",
        "  elif beta_schedule == 'linear':\n",
        "    betas = np.linspace(beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64)\n",
        "  elif beta_schedule == 'warmup10':\n",
        "    betas = _warmup_beta(beta_start, beta_end, num_diffusion_timesteps, 0.1)\n",
        "  elif beta_schedule == 'warmup50':\n",
        "    betas = _warmup_beta(beta_start, beta_end, num_diffusion_timesteps, 0.5)\n",
        "  elif beta_schedule == 'const':\n",
        "    betas = beta_end * np.ones(num_diffusion_timesteps, dtype=np.float64)\n",
        "  elif beta_schedule == 'jsd':  # 1/T, 1/(T-1), 1/(T-2), ..., 1\n",
        "    betas = 1. / np.linspace(num_diffusion_timesteps, 1, num_diffusion_timesteps, dtype=np.float64)\n",
        "  else:\n",
        "    raise NotImplementedError(beta_schedule)\n",
        "  assert betas.shape == (num_diffusion_timesteps,)\n",
        "  return betas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "sBwsLvmgi5IT"
      },
      "outputs": [],
      "source": [
        "betas= get_beta_schedule(\"warmup50\", beta1, beta2, timesteps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "uJu_yX60i5IV",
        "outputId": "923dac33-bf6f-4478-9eac-3ee9292ffefa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "print(type(betas))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "zsAII3LYi5Ia"
      },
      "outputs": [],
      "source": [
        "# construct DDPM noise schedule\n",
        "def ddpm_noise_schedule(beta1,beta2,timesteps):\n",
        "    b_t = (beta2 - beta1) * torch.linspace(0, 1, timesteps + 1, device=device) + beta1\n",
        "    a_t = 1 - b_t\n",
        "    ab_t = torch.cumsum(a_t.log(), dim=0).exp()\n",
        "    ab_t[0] = 1\n",
        "    return b_t,a_t,ab_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "Vhru858-i5Ic"
      },
      "outputs": [],
      "source": [
        "# construct model\n",
        "nn_model = Unet(3,n_feat,backbone_name='resnet34').to(device)\n",
        "\n",
        "#nn_model =SimpleCNN().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Q-zvVgpi5Ig"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "pNm6OXS4i5Iq",
        "outputId": "1ea873d2-6012-42b5-aa52-c943dc6bb08b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './wind_366X366.npy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-4e19093fd74e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load dataset and construct optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#dataset = CustomDataset(\"./sprites_1788_16x16.npy\", \"./sprite_labels_nc_1788_16x16.npy\", transform, null_context=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./wind_366X366.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./wind_label_366X366.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnull_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlrate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-e9c3d71a489e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sfilename, lfilename, transform, null_context)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnull_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msprites\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"sprite shape: {self.sprites.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './wind_366X366.npy'"
          ]
        }
      ],
      "source": [
        "# load dataset and construct optimizer\n",
        "#dataset = CustomDataset(\"./sprites_1788_16x16.npy\", \"./sprite_labels_nc_1788_16x16.npy\", transform, null_context=False)\n",
        "dataset = CustomDataset(\"./wind_366X366.npy\", \"./wind_label_366X366.npy\", transform, null_context=False)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "optim = torch.optim.AdamW(nn_model.parameters(), lr=lrate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkDD3-Bai5It"
      },
      "outputs": [],
      "source": [
        "# helper function: perturbs an image to a specified noise level\n",
        "def perturb_input(x, t, noise):\n",
        "    return ab_t.sqrt()[t, None, None, None] * x + (1 - ab_t[t, None, None, None]) * noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYw1msPii5I1"
      },
      "outputs": [],
      "source": [
        "# helper function; removes the predicted noise (but adds some noise back in to avoid collapse)\n",
        "def denoise_add_noise(beta_start,beta_end,x, t, pred_noise, z=None, ):\n",
        "    if z is None:\n",
        "        z = torch.randn_like(x)\n",
        "    b_t,a_t,ab_t=ddpm_noise_schedule(beta_start,beta_end,timesteps)\n",
        "    noise = b_t.sqrt()[t] * z\n",
        "    mean = (x - pred_noise * ((1 - a_t[t]) / (1 - ab_t[t]).sqrt())) / a_t[t].sqrt()\n",
        "    return mean + noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rerXSrH4i5I3"
      },
      "outputs": [],
      "source": [
        "# sample using standard algorithm\n",
        "@torch.no_grad()\n",
        "def sample_ddpm(n_sample, beta_start,beta_end,save_rate=20):\n",
        "    # x_T ~ N(0, 1), sample initial noise\n",
        "    samples = torch.randn(n_sample, 3, height, height).to(device)\n",
        "\n",
        "    # array to keep track of generated steps for plotting\n",
        "    intermediate = []\n",
        "    for i in range(timesteps, 0, -1):\n",
        "        print(f'sampling timestep {i:3d}', end='\\r')\n",
        "\n",
        "        # reshape time tensor\n",
        "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
        "\n",
        "        # sample some random noise to inject back in. For i = 1, don't add back in noise\n",
        "        z = torch.randn_like(samples) if i > 1 else 0\n",
        "\n",
        "        eps = nn_model(samples, t)    # predict noise e_(x_t,t)\n",
        "        samples = denoise_add_noise(beta_start,beta_end,samples, i, eps, z)\n",
        "        if i % save_rate ==0 or i==timesteps or i<8:\n",
        "            intermediate.append(samples.detach().cpu().numpy())\n",
        "\n",
        "    intermediate = np.stack(intermediate)\n",
        "    return samples, intermediate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nc8--VQji5I5"
      },
      "outputs": [],
      "source": [
        "loss_list=[]\n",
        "for i in range(40,164,4):\n",
        "    loss_list.append(np.average(np.load(save_dir+f\"loss_backbone_{i}.npy\")))\n",
        "    plt.plot(loss_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjxMVJovi5I7"
      },
      "outputs": [],
      "source": [
        "\n",
        "loss=np.load(\"loss128X128.npy\")\n",
        "loss_avg=[]\n",
        "for i in range(0,500):\n",
        "     loss_avg.append((np.average(loss[i][0:2196])))\n",
        "\n",
        "plt.plot(loss_avg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK7EXu1ji5JA"
      },
      "source": [
        "### View Epoch 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjfDF8jDi5JB"
      },
      "outputs": [],
      "source": [
        "# load in model weights and set to eval mode\n",
        "nn_model.load_state_dict(torch.load(f\"{save_dir}/model_backbone_160.pth\", map_location=device))\n",
        "nn_model.eval()\n",
        "print(\"Loaded in Model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRJtHD1Qi5JD"
      },
      "outputs": [],
      "source": [
        "# visualize samples\n",
        "plt.clf()\n",
        "\n",
        "samples, intermediate_ddpm = sample_ddpm(32,beta1,beta2)\n",
        "img=(intermediate_ddpm* 255.0).astype(np.uint8)[30][10]\n",
        "plt.imshow(img.transpose(1, 2, 0),cmap=\"viridis\", interpolation='nearest')\n",
        "plt.show()\n",
        "#animation_ddpm = plot_sample(intermediate_ddpm,32,4,save_dir, \"ani_run\", i, save=True)\n",
        "#HTML(animation_ddpm.to_jshtml())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ry8f9OFi5JF"
      },
      "outputs": [],
      "source": [
        "# construct DDPM noise schedule\n",
        "b_t = (beta2 - beta1) * torch.linspace(0, 1, timesteps + 1, device=device) + beta1\n",
        "a_t = 1 - b_t\n",
        "ab_t = torch.cumsum(a_t.log(), dim=0).exp()\n",
        "ab_t[0] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtV89s85i5JG"
      },
      "outputs": [],
      "source": [
        "def denoise_ddim(x, t, t_prev, pred_noise):\n",
        "    ab = ab_t[t]\n",
        "    ab_prev = ab_t[t_prev]\n",
        "\n",
        "    x0_pred = ab_prev.sqrt() / ab.sqrt() * (x - (1 - ab).sqrt() * pred_noise)\n",
        "    dir_xt = (1 - ab_prev).sqrt() * pred_noise\n",
        "\n",
        "    return x0_pred + dir_xt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_dOb_AGi5JH"
      },
      "outputs": [],
      "source": [
        "# sample quickly using DDIM\n",
        "@torch.no_grad()\n",
        "def sample_ddim(n_sample, n=20):\n",
        "    # x_T ~ N(0, 1), sample initial noise\n",
        "    samples = torch.randn(n_sample, 3, height, height).to(device)\n",
        "\n",
        "    # array to keep track of generated steps for plotting\n",
        "    intermediate = []\n",
        "    step_size = timesteps // n\n",
        "    for i in range(timesteps, 0, -step_size):\n",
        "        print(f'sampling timestep {i:3d}', end='\\r')\n",
        "\n",
        "        # reshape time tensor\n",
        "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
        "\n",
        "        eps = nn_model(samples, t)    # predict noise e_(x_t,t)\n",
        "        samples = denoise_ddim(samples, i, i - step_size, eps)\n",
        "        intermediate.append(samples.detach().cpu().numpy())\n",
        "\n",
        "    intermediate = np.stack(intermediate)\n",
        "    return samples, intermediate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTmua0l-i5JI"
      },
      "outputs": [],
      "source": [
        "# visualize samples\n",
        "plt.clf()\n",
        "samples, intermediate = sample_ddim(32, n=25)\n",
        "img=(intermediate* 255.0).astype(np.uint8)[20][10]\n",
        "plt.imshow(img.transpose(1, 2, 0),cmap=\"viridis\", interpolation='nearest')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "bop",
      "language": "python",
      "name": "bop"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}